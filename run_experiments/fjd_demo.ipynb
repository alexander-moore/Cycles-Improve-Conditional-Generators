{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use FJD\n",
    "\n",
    "This notebook provides a simple example of how to evaluate a conditional GAN using FJD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from fjd.fjd_metric import FJDMetric\n",
    "from fjd.embeddings import OneHotEmbedding, InceptionEmbedding\n",
    "\n",
    "import os\n",
    "os.getcwd()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    AE = Autoencoder()\n",
    "    print(AE)\n",
    "    AE.load_state_dict(torch.load('models/caegan_mnist_G.pt'))\n",
    "    with torch.no_grad():\n",
    "            codes = one_hot_embedding(torch.tensor(list(range(9)), device = 'cpu')).view(9,c_dim,1,1).float()\n",
    "            varis = torch.randn((9, v_dim,1,1), device = 'cpu') # walk from [0,...,0] to [1,...,1]\n",
    "            #print(codes.shape, varis.shape)\n",
    "            generated = .5*(AE.forward(varis, codes).cpu() + 1)\n",
    "            generated = torch.squeeze(generated)\n",
    "            #print(generated.shape)\n",
    "            for i in range(9):\n",
    "                plt.subplot(330 + 1 + i)\n",
    "                # plot raw pixel data\n",
    "                element = generated[i,:]\n",
    "                plt.imshow(element, cmap = 'gray')\n",
    "            plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute FJD we will need two data loaders: one to provide images and conditioning for the reference distribution, and a second one whose conditioning will be used to condition the GAN for creating the generated distribution. For this example we will use the CIFAR-10 dataset.\n",
    "\n",
    "When loading in reference images, it is important to normalize them between [-1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders():\n",
    "    #transform = transforms.Compose(\n",
    "    #    [transforms.ToTensor(),\n",
    "    #     transforms.Normalize(mean=(0.5, 0.5, 0.5), \n",
    "    #                          std=(0.5, 0.5, 0.5))])\n",
    "    \n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=[0.5], \n",
    "                              std=[0.5])])\n",
    "\n",
    "    train_set = MNIST(root='./data',\n",
    "                        train=True,\n",
    "                        download=True,\n",
    "                        transform=transform)\n",
    "\n",
    "    test_set = MNIST(root='./data',\n",
    "                       train=False,\n",
    "                       download=True,\n",
    "                       transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_set,\n",
    "                              batch_size=128,\n",
    "                              shuffle=True,\n",
    "                              drop_last=False)\n",
    "\n",
    "    test_loader = DataLoader(test_set,\n",
    "                             batch_size=128,\n",
    "                             shuffle=False,\n",
    "                             drop_last=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        ## Encoding: Unconditional samples\n",
    "        self.conv1 = nn.Conv2d(1, 128, 4, 2, 1) # Input: (bs, 3, img_size, img_size)\n",
    "        self.conv2 = nn.Conv2d(128, 256, 4, 2, 1, bias = False)\n",
    "        self.conv2_bn = nn.BatchNorm2d(256)\n",
    "        self.conv3 = nn.Conv2d(256, 512, 4, 2, 1, bias = False)\n",
    "        self.conv3_bn = nn.BatchNorm2d(512)\n",
    "        self.conv4 = nn.Conv2d(512, 1024, 4, 2, 1, bias = False)\n",
    "        self.conv4_bn = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        self.conv5v = nn.Conv2d(1024, v_dim, 4, 1, 0) # Output: (bs, c_dim, 1, 1)\n",
    "        self.conv5c = nn.Conv2d(1024, c_dim, 4, 1, 0) # Output, same as above: but this one to condition-space\n",
    "        \n",
    "        ## Decoding:\n",
    "        self.deconv1v = nn.ConvTranspose2d(v_dim, 1024, 4, 1, 0, bias = False) # Not sure how this looks\n",
    "        self.deconv1c = nn.ConvTranspose2d(c_dim, 1024, 4, 1, 0, bias = False) # Input: (bs, cdim+v_dim, 1, 1)\n",
    "        \n",
    "        self.deconv1_bn = nn.BatchNorm2d(1024)\n",
    "        self.deconv2 = nn.ConvTranspose2d(1024+1024, 512, 4, 2, 1, bias = False)\n",
    "        self.deconv2_bn = nn.BatchNorm2d(512)\n",
    "        self.deconv3 = nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False)\n",
    "        self.deconv3_bn = nn.BatchNorm2d(256)\n",
    "        self.deconv4 = nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False)\n",
    "        self.deconv4_bn = nn.BatchNorm2d(128)\n",
    "        self.deconv5 = nn.ConvTranspose2d(128, 1, 4, 2, 1)\n",
    "    \n",
    "    def weight_init(self):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m])\n",
    "            \n",
    "    def encode(self, x):\n",
    "        # Encode data x to 2 spaces: condition space and variance-space\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2)\n",
    "        x = F.leaky_relu(self.conv2_bn(self.conv2(x)), 0.2)\n",
    "        x = F.leaky_relu(self.conv3_bn(self.conv3(x)), 0.2)\n",
    "        x = F.leaky_relu(self.conv4_bn(self.conv4(x)), 0.2)\n",
    "        \n",
    "        v = torch.sigmoid(self.conv5v(x)) # Variance-space unif~[0,1]\n",
    "        c = torch.sigmoid(self.conv5c(x)) # this is softmax for CLASSIFICATION. Shapes3d is not 1-classif..\n",
    "        \n",
    "        return v, c\n",
    "      \n",
    "    def forward(self, v, c):\n",
    "        # This is actually conditional generation // decoding.\n",
    "        # It's beneficial to call this forward, though, for FJD calculation\n",
    "        v = self.deconv1_bn(self.deconv1v(v))\n",
    "        c = self.deconv1_bn(self.deconv1c(c))\n",
    "        x = torch.cat((v, c), dim = 1) #stack on channel dim, should be (bs, vdim+cdim, 1, 1). Not sure here\n",
    "        x = F.relu(self.deconv2_bn(self.deconv2(x)))\n",
    "        x = F.relu(self.deconv3_bn(self.deconv3(x)))\n",
    "        x = F.relu(self.deconv4_bn(self.deconv4(x)))\n",
    "        x = torch.tanh(self.deconv5(x))\n",
    "        return x\n",
    "    \n",
    "    def pass_thru(self, x):\n",
    "        v, c = self.encode(x)\n",
    "        return self.forward(v, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate a GAN we will use samples from the test set. Note that the labels from the test set are shuffled, so although the image distribution should match the reference distribution well, the conditional consistency will be very bad since most of the \"generated\" images will not match the requested conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## My GANWrapper\n",
    "class GANWrapper:\n",
    "    def __init__(self, model, model_checkpoint=None):\n",
    "        self.model = model\n",
    "        \n",
    "        if model_checkpoint is not None:\n",
    "            self.model_checkpoint = model_checkpoint\n",
    "            self.load_model()\n",
    "         \n",
    "        # model weights are loaded correctly...\n",
    "        #print(self.model.deconv1v.weight)\n",
    "\n",
    "    def load_model(self):\n",
    "        self.model.eval()  # uncomment to put in eval mode if desired\n",
    "        self.model = self.model.cuda()\n",
    "        \n",
    "        #state_dict = torch.load(self.model_checkpoint)\n",
    "        self.model.load_state_dict(torch.load('models/caegan_mnist_G.pt'))\n",
    "        \n",
    "        print('State dict freshly loaded. Now print:')\n",
    "        with torch.no_grad():\n",
    "            codes = one_hot_embedding(torch.tensor(list(range(9)), device = device)).view(9,c_dim,1,1).float()\n",
    "            varis = torch.randn((9, v_dim,1,1), device = device) # walk from [0,...,0] to [1,...,1]\n",
    "            print(codes.shape, varis.shape)\n",
    "            generated = .5*(self.model.forward(varis, codes).cpu() + 1)\n",
    "            generated = torch.squeeze(generated)\n",
    "            print(generated.shape)\n",
    "            for i in range(9):\n",
    "                plt.subplot(330 + 1 + i)\n",
    "                # plot raw pixel data\n",
    "                element = generated[i,:]\n",
    "                plt.imshow(element, cmap = 'gray')\n",
    "            plt.show()\n",
    "\n",
    "    def get_noise(self, batch_size):\n",
    "        # change the noise dimension as required\n",
    "        z = torch.randn((batch_size, v_dim, 1, 1), device = device)\n",
    "        return z\n",
    "\n",
    "    def __call__(self, y):\n",
    "        #print(y.shape) # Might need to use my one-hot, etc on this\n",
    "        #print(y[0])\n",
    "        batch_size = y.size(0)\n",
    "        #print(y)\n",
    "        #print(y.shape)\n",
    "        \n",
    "        \n",
    "\n",
    "        y = one_hot_embedding(y).view(batch_size, c_dim, 1, 1).to(device).float()\n",
    "        z = self.get_noise(batch_size)\n",
    "        #print(y.shape)\n",
    "        #print(y[0])\n",
    "        samples = self.model(z, y)\n",
    "        #print(samples.shape)\n",
    "        #print(torch.min(samples), torch.max(samples))\n",
    "        #plt.imshow(samples[0].cpu().squeeze(), cmap = 'gray')\n",
    "        #plt.show()\n",
    "        #samples = self.model(z)\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to accomodate a wide variety of model configurations, we use a GAN wrapper to standardize model inputs and outputs. Each model is expected to take as input a set of conditions _y_, and return a corresponding set of generated samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_embedding(labels):\n",
    "    #y = torch.eye(num_classes)\n",
    "    #return y[labels]\n",
    "    #return torch.nn.functional.one_hot(labels)[:,1:]\n",
    "    \n",
    "    labels = torch.nn.functional.one_hot(torch.tensor(labels).to(torch.int64), num_classes = c_dim)\n",
    "    return torch.squeeze(labels)\n",
    "\n",
    "#class GANWrapper:\n",
    "#    def __init__(self, model, model_checkpoint=None):\n",
    "#        self.model = model\n",
    "#        \n",
    "#        if model_checkpoint is not None:\n",
    "#            self.model_checkpoint = model_checkpoint\n",
    "#            self.load_model()#\n",
    "#\n",
    "#    def load_model(self):\n",
    "#        # self.model.eval()  # uncomment to put in eval mode if desired\n",
    "#        self.model = self.model.cuda()#\n",
    "#\n",
    "#        state_dict = torch.load(self.model_checkpoint)\n",
    "#        self.model.load_state_dict(state_dict)##\n",
    "\n",
    "#    def get_noise(self, batch_size):\n",
    "#        # change the noise dimension as required\n",
    "#        z = torch.cuda.FloatTensor(batch_size, 128).normal_()\n",
    "#        return z\n",
    "\n",
    "#    def __call__(self, y):\n",
    "#        #y = one_hot_embedding(y)\n",
    "#        batch_size = y.size(0)\n",
    "#        z = self.get_noise(batch_size)\n",
    "#        samples = self.model(z, y)\n",
    "#        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FJDMetric object handles embedding the images and conditioning, the computation of the reference distribution and generated distribution statistics, the scaling of the conditioning component with alpha, and the calculation of FJD. It requires several inputs:\n",
    "\n",
    "1. **gan** - A GAN model which takes as input conditioning and yields image samples as output.  \n",
    "2. **reference_loader** - A data loader for the reference distribution, which yields image-condition pairs.  \n",
    "3. **condition_loader** - A data loader for the generated distribution, which yields image-condition pairs. Images are ignored, and the conditioning is used as input to the GAN.  \n",
    "4. **image_embedding** - An image embedding function. This will almost always be the InceptionEmbedding.  \n",
    "5. **condition_embedding** - A conditioning embedding function. As we are dealing with class conditioning in this example, we will use one-hot encoding.\n",
    "\n",
    "Other options:\n",
    "* **save_reference_stats** - Indicates whether the statistics of the reference distribution should be saved to the path provided in **reference_stats_path**. This can speed up computation of FJD if the same reference set is used for multiple evaluations.\n",
    "* **samples_per_condition** - Indicates the number of images that will be generated for each condition drawn from the condition loader. This may be useful if there are very few samples in the conditioning dataset, or to emphasize intra-conditioning diversity when calculating FJD.\n",
    "* **cuda** - If True, indicates that the GPU accelerated version of FJD should be used. This version should be considerably faster than the CPU version, but may be slightly more unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SuspiciouslyGoodGAN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-9fe22c427f71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0minception_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInceptionEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparallel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0monehot_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOneHotEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mgan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSuspiciouslyGoodGAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m#gan = Autoencoder()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#params = 'models/cgan_mnist_G.pt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SuspiciouslyGoodGAN' is not defined"
     ]
    }
   ],
   "source": [
    "z_dim = 100\n",
    "v_dim = 100\n",
    "c_dim = 10\n",
    "\n",
    "train_loader, test_loader = get_dataloaders()\n",
    "inception_embedding = InceptionEmbedding(parallel=False)\n",
    "onehot_embedding = OneHotEmbedding(num_classes=10)\n",
    "gan = SuspiciouslyGoodGAN()\n",
    "#gan = Autoencoder()\n",
    "#params = 'models/cgan_mnist_G.pt'\n",
    "params = 'models/caegan_mnist_G.pt'\n",
    "gan = GANWrapper(gan, params)\n",
    "\n",
    "fjd_metric = FJDMetric(gan=gan,\n",
    "                       reference_loader=train_loader, #mnist train\n",
    "                       condition_loader=test_loader, #mnist test\n",
    "                       image_embedding=inception_embedding, #dont change\n",
    "                       condition_embedding=onehot_embedding,\n",
    "                       reference_stats_path='datasets/cifar_train_stats.npz',\n",
    "                       save_reference_stats=True,\n",
    "                       samples_per_condition=6,\n",
    "                       cuda=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the FJD object is initialized, FID and FJD can be calculated by calling **get_fid** or **get_fjd**.  By default, the alpha value used to weight the conditional component of FJD is selected to be the ratio between the average L2 norm of the image embedding and conditioning embedding.  \n",
    "\n",
    "We see in this example that even though our \"GAN\" gets a very good FID score due to the generated image distribution being very close to the reference image distribution, its FJD score is very bad, as the model lacks any conditional consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid = fjd_metric.get_fid()\n",
    "fjd = fjd_metric.get_fjd()\n",
    "print('FID: ', fid)\n",
    "print('FJD: ', fjd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize how FJD changes as we increase the weighting on the conditional component, we can evaluate it at a range of alpha values using the **sweep_alpha** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = fjd_metric.alpha\n",
    "alphas = [0, 1, 2, 4, 8, 16, 32]\n",
    "fjds = fjd_metric.sweep_alpha(alphas)\n",
    "\n",
    "plt.plot(alphas, fjds, label='FJD', linewidth=3)\n",
    "plt.plot(alphas, [fid]*len(alphas), label='FID', linewidth=3)\n",
    "plt.axvline(x=alpha, c='black', label=r'Suggested $\\alpha$', linewidth=2)\n",
    "plt.xlabel(r'$\\alpha$')\n",
    "plt.ylabel('Distance')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
